{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adronz/Needle-Segmentation/blob/main/NeedleSAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAAHyovRP6D6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoImageProcessor, ViTMAEForPreTraining, ViTFeatureExtractor, ViTImageProcessor\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim\n",
        "# import torch.nn as nn\n",
        "from PIL import Image\n",
        "\n",
        "#for trainer\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258,
          "referenced_widgets": [
            "d3a3f28cef004234873c3ee5f8a5ff82",
            "f33c47e0c3b94206b6d4d29a5b1546c7",
            "84a90c79e5fa4c7491db469cc5a6ba49",
            "90ded4f9d6dd4ae29fd67ab2eb0e0548",
            "020f88c04e3c421cb07c7391ac5c6044",
            "392521e8a69f4802a658e5143d9fdafb",
            "a83f44d4611a4376bfd1045bb1ff7748",
            "0d52b61b05214308ab35b4f2af97ab35",
            "7e63f36301d24f09bd4c33274571ac4e",
            "05e46d1f168344499eeb0f3390d0c6a2",
            "d6140c99a6da44d989e5a2361c1da1a6",
            "b4e0176f9ee44426bb80f10eb51b927e",
            "2707b3fd597447d2905288caf4bee162",
            "1c90cec486c647a5b62798d993bd672b",
            "f2985b6f4f534f7ca2e8a977450c8382",
            "035e328e3cd145c7aa0a4e87f5721e59",
            "b6061f80eab24af68d312d88e60e9bc5",
            "84d4f33e2a2949bea63c91aebf4b5d53",
            "e4fa1b8848b14084bebd85a59d348a87",
            "16dac8db70ff4d5997f147abbe476422",
            "fe79260794a0453eba0bd3c6fca7b046",
            "bbf0b892752a4ee5815a31ed7520e513",
            "0034672b0aeb4374b5054443eb560d6c",
            "67433b8cf412451eb34c07d952acfe9c",
            "ab36c74405264af997ca3d182d13a266",
            "cb3aceef697b47c98f71e99770b387cf",
            "4c873ae80b4f4c58bc8678cfc5b9c014",
            "74f7ac22cad3479595d23e771e75cdfa",
            "dbb13d1948884ee698efaebd1a6babe8",
            "ec7876c265e8428bbca6b77c0e884212",
            "15bbb28521734778a12fe866358b2542",
            "43576e361a0a41ad866fc5d90da9e327",
            "23e4802fb1f84d198cc22671ba234220"
          ]
        },
        "id": "DxnDeA8cPnRg",
        "outputId": "0026a931-c33c-4b1d-90e5-b5e9bf4186d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device is cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3a3f28cef004234873c3ee5f8a5ff82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/6.52k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4e0176f9ee44426bb80f10eb51b927e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0034672b0aeb4374b5054443eb560d6c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import SamModel, SamProcessor\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f'device is {device}')\n",
        "\n",
        "processor = SamProcessor.from_pretrained(\"wanglab/medsam-vit-base\")\n",
        "model = SamModel.from_pretrained(\"wanglab/medsam-vit-base\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIo5FpM0QDC5",
        "outputId": "5dbd7540-007c-4e28-acfc-bb69f12ca2b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = '/content/gdrive/MyDrive/needleSAM/trainImages'\n",
        "mask_dir = '/content/gdrive/MyDrive/needleSAM/trainMasks'"
      ],
      "metadata": {
        "id": "5s0PgijCiDmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ffWOvA-R39D"
      },
      "outputs": [],
      "source": [
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ2ba2VESLhp"
      },
      "outputs": [],
      "source": [
        "def get_bounding_box(ground_truth_map):\n",
        "  # get bounding box from mask\n",
        "  y_indices, x_indices = np.where(ground_truth_map > 0)\n",
        "  x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
        "  y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
        "  # add perturbation to bounding box coordinates\n",
        "  H, W = ground_truth_map.shape\n",
        "  x_min = max(0, x_min - np.random.randint(0, 20))\n",
        "  x_max = min(W, x_max + np.random.randint(0, 20))\n",
        "  y_min = max(0, y_min - np.random.randint(0, 20))\n",
        "  y_max = min(H, y_max + np.random.randint(0, 20))\n",
        "  bbox = [x_min, y_min, x_max, y_max]\n",
        "\n",
        "  return bbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVMnzi_JSgA7"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "class SAMDataset(Dataset):\n",
        "  def __init__(self, image_path, mask_path, processor, transform=None):\n",
        "    self.image_dir = image_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "\n",
        "\n",
        "    # List all files in the image_dir assuming files are named as '1344.jpg'\n",
        "    self.filenames = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
        "#     self.filenames.sort()  # Sort files to maintain order\n",
        "\n",
        "\n",
        "    self.processor = processor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.filenames)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    #FROM NEEDLE DATASET CLASS\n",
        "    img_name = self.filenames[idx]\n",
        "    img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "    # Corresponding mask file, assuming mask files are named '1342_mask.jpg'\n",
        "    mask_name = img_name.split('.')[0] + '_mask.png'\n",
        "    mask_path = os.path.join(self.mask_dir, mask_name)\n",
        "\n",
        "\n",
        "    # item = self.dataset[idx]\n",
        "    image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    #for normalization of image\n",
        "    m, s = np.mean(image, axis=(0, 1)), np.std(image, axis=(0, 1))\n",
        "\n",
        "\n",
        "    ###########\n",
        "    ###########\n",
        "\n",
        "    #preprocess images\n",
        "    image = image.resize((256, 256))\n",
        "    image_np = np.array(image)\n",
        "    image_tensor = torch.from_numpy(image_np).float()\n",
        "    image_tensor = image_tensor.permute(1,2,0)\n",
        "    norm_image = transforms.Normalize(mean=m, std=s)\n",
        "\n",
        "    #preprocess masks\n",
        "    ground_truth_mask = Image.open(mask_path).convert('L')\n",
        "    ground_truth_mask = ground_truth_mask.resize((256, 256))\n",
        "\n",
        "    ground_truth_mask = np.array(ground_truth_mask)\n",
        "\n",
        "\n",
        "    # get bounding box prompt\n",
        "    if np.any(ground_truth_mask):\n",
        "    # If the mask is not empty, get bounding box\n",
        "       prompt = get_bounding_box(ground_truth_mask)\n",
        "    else:\n",
        "    # Define a default prompt or handling for empty masks\n",
        "       prompt = [0, 0, 1, 1]  # Example: a minimal default box or similar appropriate value\n",
        "\n",
        "\n",
        "\n",
        "    # prepare image and prompt for the model\n",
        "    inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "    # remove batch dimension which the processor adds by default\n",
        "    inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
        "\n",
        "    # add ground truth segmentation\n",
        "    inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
        "\n",
        "\n",
        "    return inputs\n",
        "\n",
        "# Create dataset\n",
        "dataset = SAMDataset(image_dir, mask_dir, processor)\n",
        "\n",
        "# Split into train and validation datasets (e.g., 80% train, 20% validation)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=5, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r6bkxzjfXan",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "outputId": "8b6c4ba8-4926-4ea2-d28a-779d675a0fb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'pyarrow.lib' has no attribute 'ListViewType'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-16793eeb1093>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip -q install datasets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimage_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmask_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.20.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuilderConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_writer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptimizedTypedSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msanitize_patterns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_reader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthread_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/parquet/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# flake8: noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parquet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     raise ImportError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/_parquet.pyx\u001b[0m in \u001b[0;36minit pyarrow._parquet\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pyarrow.lib' has no attribute 'ListViewType'"
          ]
        }
      ],
      "source": [
        "# !pip -q install datasets\n",
        "# from datasets import load_dataset\n",
        "\n",
        "# image_data = load_dataset(image_dir, split='train')\n",
        "# mask_data = load_dataset(mask_dir, split='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbqSa3bYarIh"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBuxoq2TS9Fn"
      },
      "outputs": [],
      "source": [
        "# train_dataset = SAMDataset(image_dir, mask_dir, processor=processor)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-XWfaNnhon_"
      },
      "outputs": [],
      "source": [
        "# train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbD8Ip3jhYa2"
      },
      "outputs": [],
      "source": [
        "# train_dataset[0]['ground_truth_mask'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5qoe5YbWT8j"
      },
      "outputs": [],
      "source": [
        "# example = train_dataset[0]\n",
        "# for k,v in example.items():\n",
        "#   print(k,v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6XErvr-TRnr"
      },
      "outputs": [],
      "source": [
        "# train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jr98qaJV9EV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd825f19-3350-4c6b-bef2-536ddb643f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pixel_values torch.Size([5, 3, 1024, 1024])\n",
            "original_sizes torch.Size([5, 2])\n",
            "reshaped_input_sizes torch.Size([5, 2])\n",
            "input_boxes torch.Size([5, 1, 4])\n",
            "ground_truth_mask torch.Size([5, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "for k, v in batch.items():\n",
        "    print(k, v.shape)  # Check the output shapes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0I_yRFpThOD"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
        "    param.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDL9dTZeTpmw"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "!pip install -q monai\n",
        "import monai\n",
        "# Note: Hyperparameter tuning could improve performance here\n",
        "optimizer = AdamW(model.mask_decoder.parameters(), lr=0.001)\n",
        "\n",
        "#try other_act = nn.tanh and sigmoid = false\n",
        "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(predicted_masks, ground_truth_masks):\n",
        "    predicted_masks = (predicted_masks > 0.5).float()\n",
        "    ground_truth_masks = (ground_truth_masks > 0.5).float()\n",
        "\n",
        "    true_positive = (predicted_masks * ground_truth_masks).sum().item()\n",
        "    false_positive = (predicted_masks * (1 - ground_truth_masks)).sum().item()\n",
        "    true_negative = ((1 - predicted_masks) * (1 - ground_truth_masks)).sum().item()\n",
        "    false_negative = ((1 - predicted_masks) * ground_truth_masks).sum().item()\n",
        "\n",
        "    accuracy = (true_positive + true_negative) / (true_positive + false_positive + true_negative + false_negative)\n",
        "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0.0\n",
        "\n",
        "    return accuracy, precision"
      ],
      "metadata": {
        "id": "XWAov9LVhlTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from statistics import mean\n",
        "import torch\n",
        "from torch.nn.functional import threshold, normalize\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Set up TensorBoard writer\n",
        "writer = SummaryWriter()\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):a\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
        "        # forward pass\n",
        "        outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
        "                        input_boxes=batch[\"input_boxes\"].to(device),\n",
        "                        multimask_output=False)\n",
        "\n",
        "        # compute loss\n",
        "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
        "        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device) / 255.0\n",
        "        loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
        "\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "    # Calculate mean training loss\n",
        "    mean_train_loss = mean(train_losses)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_dataloader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n",
        "            outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
        "                            input_boxes=batch[\"input_boxes\"].to(device),\n",
        "                            multimask_output=False)\n",
        "\n",
        "            predicted_masks = outputs.pred_masks.squeeze(1)\n",
        "            ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device) / 255.0\n",
        "            val_loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "    mean_val_loss = mean(val_losses)\n",
        "\n",
        "    # Log losses to TensorBoard\n",
        "    writer.add_scalar('Loss/Train', mean_train_loss, epoch)\n",
        "    writer.add_scalar('Loss/Validation', mean_val_loss, epoch)\n",
        "\n",
        "    print(f'EPOCH {epoch+1}/{num_epochs}')\n",
        "    print(f'Mean training loss: {mean_train_loss}')\n",
        "    print(f'Mean validation loss: {mean_val_loss}')\n",
        "\n",
        "# Close the TensorBoard writer\n",
        "writer.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F8PJRbTIXJie",
        "outputId": "ff8e323b-09b7-43af-f61f-dbbee4236394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/20: 100%|██████████| 81/81 [07:40<00:00,  5.68s/it]\n",
            "Validation Epoch 1/20: 100%|██████████| 21/21 [01:53<00:00,  5.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1/20\n",
            "Mean training loss: 0.9230124350683189\n",
            "Mean validation loss: 0.48608317261650447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 81/81 [03:36<00:00,  2.67s/it]\n",
            "Validation Epoch 2/20: 100%|██████████| 21/21 [00:53<00:00,  2.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 2/20\n",
            "Mean training loss: 0.43547107584560796\n",
            "Mean validation loss: 0.3360822924545833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 81/81 [03:35<00:00,  2.66s/it]\n",
            "Validation Epoch 3/20: 100%|██████████| 21/21 [00:53<00:00,  2.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 3/20\n",
            "Mean training loss: 0.36237849672267464\n",
            "Mean validation loss: 0.3456457534006664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/20: 100%|██████████| 81/81 [03:35<00:00,  2.66s/it]\n",
            "Validation Epoch 4/20: 100%|██████████| 21/21 [00:53<00:00,  2.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 4/20\n",
            "Mean training loss: 0.32086432725191116\n",
            "Mean validation loss: 0.3070270951305117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5/20: 100%|██████████| 81/81 [03:35<00:00,  2.66s/it]\n",
            "Validation Epoch 5/20: 100%|██████████| 21/21 [00:53<00:00,  2.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 5/20\n",
            "Mean training loss: 0.30031332520791043\n",
            "Mean validation loss: 0.2972636563437326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6/20: 100%|██████████| 81/81 [03:35<00:00,  2.66s/it]\n",
            "Validation Epoch 6/20: 100%|██████████| 21/21 [00:53<00:00,  2.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 6/20\n",
            "Mean training loss: 0.29210912095911706\n",
            "Mean validation loss: 0.28204979605617975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7/20: 100%|██████████| 81/81 [03:35<00:00,  2.66s/it]\n",
            "Validation Epoch 7/20: 100%|██████████| 21/21 [00:52<00:00,  2.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 7/20\n",
            "Mean training loss: 0.2941251787598486\n",
            "Mean validation loss: 0.2911106106780824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8/20: 100%|██████████| 81/81 [03:35<00:00,  2.66s/it]\n",
            "Validation Epoch 8/20: 100%|██████████| 21/21 [00:52<00:00,  2.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 8/20\n",
            "Mean training loss: 0.2801728407265963\n",
            "Mean validation loss: 0.28732989941324505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9/20: 100%|██████████| 81/81 [03:35<00:00,  2.67s/it]\n",
            "Validation Epoch 9/20: 100%|██████████| 21/21 [00:53<00:00,  2.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 9/20\n",
            "Mean training loss: 0.26910936814031483\n",
            "Mean validation loss: 0.2765322313422248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10/20: 100%|██████████| 81/81 [03:35<00:00,  2.66s/it]\n",
            "Validation Epoch 10/20: 100%|██████████| 21/21 [00:53<00:00,  2.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 10/20\n",
            "Mean training loss: 0.26405734688411525\n",
            "Mean validation loss: 0.27602027924287886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 11/20: 100%|██████████| 81/81 [03:36<00:00,  2.67s/it]\n",
            "Validation Epoch 11/20: 100%|██████████| 21/21 [00:52<00:00,  2.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 11/20\n",
            "Mean training loss: 0.2602544726550947\n",
            "Mean validation loss: 0.28308271297386717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 12/20: 100%|██████████| 81/81 [03:36<00:00,  2.67s/it]\n",
            "Validation Epoch 12/20: 100%|██████████| 21/21 [00:52<00:00,  2.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 12/20\n",
            "Mean training loss: 0.25576976116424727\n",
            "Mean validation loss: 0.2876344067709787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 13/20: 100%|██████████| 81/81 [03:35<00:00,  2.66s/it]\n",
            "Validation Epoch 13/20: 100%|██████████| 21/21 [00:53<00:00,  2.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 13/20\n",
            "Mean training loss: 0.25150238575390826\n",
            "Mean validation loss: 0.26961806700343177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 14/20: 100%|██████████| 81/81 [03:36<00:00,  2.67s/it]\n",
            "Validation Epoch 14/20: 100%|██████████| 21/21 [00:52<00:00,  2.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 14/20\n",
            "Mean training loss: 0.24993312956742297\n",
            "Mean validation loss: 0.28089521470524015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 15/20: 100%|██████████| 81/81 [03:35<00:00,  2.67s/it]\n",
            "Validation Epoch 15/20: 100%|██████████| 21/21 [00:52<00:00,  2.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 15/20\n",
            "Mean training loss: 0.23787078555719351\n",
            "Mean validation loss: 0.2744516460668473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 16/20: 100%|██████████| 81/81 [03:35<00:00,  2.67s/it]\n",
            "Validation Epoch 16/20: 100%|██████████| 21/21 [00:53<00:00,  2.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 16/20\n",
            "Mean training loss: 0.240623627823812\n",
            "Mean validation loss: 0.2737192288041115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 17/20: 100%|██████████| 81/81 [03:35<00:00,  2.66s/it]\n",
            "Validation Epoch 17/20: 100%|██████████| 21/21 [00:53<00:00,  2.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 17/20\n",
            "Mean training loss: 0.2328323929766078\n",
            "Mean validation loss: 0.29640544880004155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 18/20: 100%|██████████| 81/81 [03:35<00:00,  2.66s/it]\n",
            "Validation Epoch 18/20: 100%|██████████| 21/21 [00:52<00:00,  2.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 18/20\n",
            "Mean training loss: 0.23961860125447498\n",
            "Mean validation loss: 0.26170975253695533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 19/20:   5%|▍         | 4/81 [00:13<04:11,  3.27s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 3.75 GiB. GPU ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-0c693a64eef2>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Training Epoch {epoch+1}/{num_epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n\u001b[0m\u001b[1;32m     22\u001b[0m                         \u001b[0minput_boxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                         multimask_output=False)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/sam/modeling_sam.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, input_points, input_labels, input_boxes, input_masks, image_embeddings, multimask_output, attention_similarity, target_embedding, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpixel_values\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1355\u001b[0;31m             vision_outputs = self.vision_encoder(\n\u001b[0m\u001b[1;32m   1356\u001b[0m                 \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/sam/modeling_sam.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                 )\n\u001b[1;32m   1042\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m                 \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/sam/modeling_sam.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         hidden_states, attn_weights = self.attn(\n\u001b[0m\u001b[1;32m    937\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/sam/modeling_sam.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_rel_pos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             attn_weights = self.add_decomposed_rel_pos(\n\u001b[0m\u001b[1;32m    840\u001b[0m                 \u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrel_pos_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrel_pos_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/sam/modeling_sam.py\u001b[0m in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(self, attn, query, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0mrel_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bhwc,wkc->bhwk\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshaped_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_position_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrel_h\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrel_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_height\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mquery_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_height\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkey_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.75 GiB. GPU "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Assuming `model` is your trained model\n",
        "# Define the path to save the model\n",
        "model_path = \"NeedleSAM.pth\"\n",
        "\n",
        "# Save the model's state dictionary\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")\n",
        "\n",
        "# Download the model file to your local machine\n",
        "files.download(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "Gwj0WMLUFYD-",
        "outputId": "4e920785-7255-40fb-a7ef-81f2c96aa91b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e2964dc48eef>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Save the model's state dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model saved to {model_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "4klzkN-wc-Ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txzglDGwT8UY"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlfnaGz-UHld"
      },
      "outputs": [],
      "source": [
        "# let's take a random training example\n",
        "idx = 0\n",
        "\n",
        "# load image\n",
        "image = image_data[idx][\"image\"].resize((256,256)).convert('RGB')\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_FrttnyhYa3"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "for k, v in batch.items():\n",
        "    print(k, v.shape)  # Check the output shapes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqnqAFF5UPPs"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "model.to(device)\n",
        "# forward pass\n",
        "with torch.no_grad():\n",
        "    outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
        "                      input_boxes=batch[\"input_boxes\"].to(device),\n",
        "                      multimask_output=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch['ground_truth_mask'][2].shape"
      ],
      "metadata": {
        "id": "2lHQuHkQ7k7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = outputs.pred_masks.shape[0]\n",
        "ground_truth_mask = batch[\"ground_truth_mask\"]\n",
        "# medsam_seg_probs = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
        "medsam_seg_probs = outputs.pred_masks.squeeze(1)\n",
        "\n",
        "for i in range(batch_size):\n",
        "    # Get the i-th original image, predicted mask, and ground truth mask\n",
        "    original_image = batch[\"pixel_values\"][i].permute(1, 2, 0).detach().cpu().numpy()\n",
        "    medsam_seg_prob = medsam_seg_probs[i].squeeze(0).detach().cpu().numpy()\n",
        "    # ground_truth_mask = ground_truth_mask[i].squeeze().detach().cpu().numpy()\n",
        "    ground_truth_mask = batch['ground_truth_mask'][i]\n",
        "    # Convert soft mask to hard mask\n",
        "    medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
        "\n",
        "    # Check the shape to ensure it is correct for visualization\n",
        "    print(f\"Shape of original image {i}: {original_image.shape}\")  # Should be (height, width, channels)\n",
        "    print(f\"Shape of predicted mask {i}: {medsam_seg.shape}\")  # Should be (height, width)\n",
        "    print(f\"Shape of ground truth mask {i}: {ground_truth_mask.shape}\")  # Should be (height, width)\n",
        "\n",
        "    # Plot the original image, predicted mask, and ground truth mask side by side\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    axes[0].imshow(original_image)\n",
        "    axes[0].set_title(f'Original Image {i}')\n",
        "    axes[0].axis('off')  # Hide axes for a cleaner look\n",
        "\n",
        "    axes[1].imshow(medsam_seg, cmap='gray')\n",
        "    axes[1].set_title(f'Predicted Mask {i}')\n",
        "    axes[1].axis('off')  # Hide axes for a cleaner look\n",
        "\n",
        "    axes[2].imshow(ground_truth_mask, cmap='gray')\n",
        "    axes[2].set_title(f'Ground Truth Mask {i}')\n",
        "    axes[2].axis('off')  # Hide axes for a cleaner look\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ovJKd4WAhpHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh7xvyYahYa3"
      },
      "source": [
        "## Generalize to test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDaYiZ_3hYa3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "input_points (torch.FloatTensor of shape (batch_size, num_points, 2)) —\n",
        "Input 2D spatial points, this is used by the prompt encoder to encode the prompt.\n",
        "Generally yields to much better results. The points can be obtained by passing a\n",
        "list of list of list to the processor that will create corresponding torch tensors\n",
        "of dimension 4. The first dimension is the image batch size, the second dimension\n",
        "is the point batch size (i.e. how many segmentation masks do we want the model to\n",
        "predict per input point), the third dimension is the number of points per segmentation\n",
        "mask (it is possible to pass multiple points for a single mask), and the last dimension\n",
        "is the x (vertical) and y (horizontal) coordinates of the point. If a different number\n",
        "of points is passed either for each image, or for each mask, the processor will create\n",
        "“PAD” points that will correspond to the (0, 0) coordinate, and the computation of the\n",
        "embedding will be skipped for these points using the labels.\n",
        "\n",
        "\"\"\"\n",
        "# Define the size of your array\n",
        "array_size = 256\n",
        "\n",
        "# Define the size of your grid\n",
        "grid_size = 10\n",
        "\n",
        "# Generate the grid points\n",
        "x = np.linspace(0, array_size-1, grid_size)\n",
        "y = np.linspace(0, array_size-1, grid_size)\n",
        "\n",
        "# Generate a grid of coordinates\n",
        "xv, yv = np.meshgrid(x, y)\n",
        "\n",
        "# Convert the numpy arrays to lists\n",
        "xv_list = xv.tolist()\n",
        "yv_list = yv.tolist()\n",
        "# Combine the x and y coordinates into a list of list of lists\n",
        "input_points = [[[int(x), int(y)] for x, y in zip(x_row, y_row)] for x_row, y_row in zip(xv_list, yv_list)]\n",
        "\n",
        "#We need to reshape our nxn grid to the expected shape of the input_points tensor\n",
        "# (batch_size, point_batch_size, num_points_per_image, 2),\n",
        "# where the last dimension of 2 represents the x and y coordinates of each point.\n",
        "#batch_size: The number of images you're processing at once.\n",
        "#point_batch_size: The number of point sets you have for each image.\n",
        "#num_points_per_image: The number of points in each set.\n",
        "input_points = torch.tensor(input_points).view(1, 1, grid_size*grid_size, 2)\n",
        "\n",
        "\n",
        "print(np.array(input_points).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQJnCHjhhYa4"
      },
      "outputs": [],
      "source": [
        "image_data[8]['image']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0d7yF9khYa4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Select a random patch for segmentation\n",
        "\n",
        "# # Compute the total number of 256x256 arrays\n",
        "# #num_arrays = patches.shape[0] * patches.shape[1]\n",
        "# # Select a random index\n",
        "# #index = np.random.choice(num_arrays)\n",
        "# # Compute the indices in the original array\n",
        "# #i = index // patches.shape[1]\n",
        "# #j = index % patches.shape[1]\n",
        "\n",
        "# #Or pick a specific patch for study.\n",
        "# i, j = 1, 2\n",
        "\n",
        "# # Selectelected patch for segmentation\n",
        "# random_array = patches[i, j]\n",
        "\n",
        "\n",
        "# single_patch = Image.fromarray(random_array)\n",
        "# # prepare image for the model\n",
        "\n",
        "# #First try without providing any prompt (no bounding box or input_points)\n",
        "# #inputs = processor(single_patch, return_tensors=\"pt\")\n",
        "# #Now try with bounding boxes. Remember to uncomment.\n",
        "\n",
        "\n",
        "image = image_data[8]['image']\n",
        "image = image.convert(\"RGB\")\n",
        "m, s = np.mean(image, axis=(0, 1)), np.std(image, axis=(0, 1))\n",
        "image = image.resize((256, 256))\n",
        "image_np = np.array(image)\n",
        "image_tensor = torch.from_numpy(image_np).float()\n",
        "# image_tensor = image_tensor.permute(1,2,0)\n",
        "print(image_tensor.shape)\n",
        "\n",
        "# norm_image = transforms.Normalize(mean=m, std=s)\n",
        "#We will just use a single image instead of patchifying\n",
        "inputs = processor(image_tensor, input_points=input_points, return_tensors=\"pt\")\n",
        "\n",
        "# Move the input tensor to the GPU if it's not already there\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# forward pass\n",
        "with torch.no_grad():\n",
        "  outputs = model(**inputs, multimask_output=False)\n",
        "\n",
        "# apply sigmoid\n",
        "single_patch_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
        "# convert soft mask to hard mask\n",
        "single_patch_prob = single_patch_prob.cpu().numpy().squeeze()\n",
        "single_patch_prediction = (single_patch_prob > 0.5).astype(np.uint8)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Plot the first image on the left\n",
        "axes[0].imshow(np.array(image), cmap='gray')  # Assuming the first image is grayscale\n",
        "axes[0].set_title(\"Image\")\n",
        "\n",
        "# Plot the second image on the right\n",
        "axes[1].imshow(single_patch_prob)  # Assuming the second image is grayscale\n",
        "axes[1].set_title(\"Probability Map\")\n",
        "\n",
        "# Plot the second image on the right\n",
        "axes[2].imshow(single_patch_prediction, cmap='gray')  # Assuming the second image is grayscale\n",
        "axes[2].set_title(\"Prediction\")\n",
        "\n",
        "# Hide axis ticks and labels\n",
        "for ax in axes:\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "\n",
        "# Display the images side by side\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unmount Google Drive if already mounted\n",
        "drive.flush_and_unmount()\n",
        "\n",
        "# Wait for a few seconds to ensure the unmount process completes\n",
        "import time\n",
        "time.sleep(5)\n",
        "\n",
        "# Remount Google Drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "z5S_3sB-hfiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = '/content/gdrive/MyDrive/testImages'"
      ],
      "metadata": {
        "id": "hAn2mrYJi2Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SortedImageDataset(Dataset):\n",
        "    def __init__(self, folder_path):\n",
        "        self.folder_path = folder_path\n",
        "        self.image_files = sorted([f for f in os.listdir(folder_path) if f.lower().endswith('.png')], key=self.extract_file_number)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.folder_path, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Extract the file number from the filename\n",
        "        file_number = self.extract_file_number(img_name)\n",
        "\n",
        "        return file_number, image\n",
        "\n",
        "    def extract_file_number(self, filename):\n",
        "        # Extract the numerical part of the filename\n",
        "        return int(os.path.splitext(filename)[0])"
      ],
      "metadata": {
        "id": "dRhrUOLQmz2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_image_dataset = SortedImageDataset(folder_path=test_dir)\n",
        "test_dataloader = DataLoader(sorted_image_dataset, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "c86a31Nsm2p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Produce test image masks"
      ],
      "metadata": {
        "id": "3JyNyODMjrTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for file_number, input_image in test_dataloader:\n",
        "      print(f\"Processing file number: {file_number.item()}\")\n",
        "      image = input_image\n",
        "      image = image.convert(\"RGB\")\n",
        "      m, s = np.mean(image, axis=(0, 1)), np.std(image, axis=(0, 1))\n",
        "      image = image.resize((256, 256))\n",
        "      image_np = np.array(image)\n",
        "      image_tensor = torch.from_numpy(image_np).float()\n",
        "      # image_tensor = image_tensor.permute(1,2,0)\n",
        "      # print(image_tensor.shape)\n",
        "      # norm_image = transforms.Normalize(mean=m, std=s)\n",
        "      #We will just use a single image instead of patchifying\n",
        "\n",
        "      inputs = processor(image_tensor, input_points=input_points, return_tensors=\"pt\")\n",
        "\n",
        "      # Move the input tensor to the GPU if it's not already there\n",
        "      inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "      outputs = model(**inputs, multimask_output=False)\n",
        "\n",
        "\n",
        "      # Assume the model output is a single channel mask (e.g., binary or grayscale)\n",
        "      mask = outputs.squeeze().cpu().numpy()  # Remove batch dimension and move to CPU\n",
        "      # Convert mask to an image\n",
        "      mask = (mask * 255).astype(np.uint8)  # Scale to [0, 255] and convert to uint8\n",
        "      mask_image = Image.fromarray(mask)\n",
        "\n",
        "      # Save the mask image\n",
        "      mask_image.save(os.path.join('/Users/AdrianHanson/Desktop/Imaging Informatics/output_masks', f'{file_numbe.item()}.png'))\n",
        "\n",
        "      print(\"Processing complete. Masks saved to\", '/Users/AdrianHanson/Desktop/Imaging Informatics/output_masks')\n"
      ],
      "metadata": {
        "id": "mAcgc9Y43_pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mAuB_V5PS3MK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d3a3f28cef004234873c3ee5f8a5ff82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f33c47e0c3b94206b6d4d29a5b1546c7",
              "IPY_MODEL_84a90c79e5fa4c7491db469cc5a6ba49",
              "IPY_MODEL_90ded4f9d6dd4ae29fd67ab2eb0e0548"
            ],
            "layout": "IPY_MODEL_020f88c04e3c421cb07c7391ac5c6044"
          }
        },
        "f33c47e0c3b94206b6d4d29a5b1546c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_392521e8a69f4802a658e5143d9fdafb",
            "placeholder": "​",
            "style": "IPY_MODEL_a83f44d4611a4376bfd1045bb1ff7748",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "84a90c79e5fa4c7491db469cc5a6ba49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d52b61b05214308ab35b4f2af97ab35",
            "max": 466,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e63f36301d24f09bd4c33274571ac4e",
            "value": 466
          }
        },
        "90ded4f9d6dd4ae29fd67ab2eb0e0548": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05e46d1f168344499eeb0f3390d0c6a2",
            "placeholder": "​",
            "style": "IPY_MODEL_d6140c99a6da44d989e5a2361c1da1a6",
            "value": " 466/466 [00:00&lt;00:00, 15.5kB/s]"
          }
        },
        "020f88c04e3c421cb07c7391ac5c6044": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "392521e8a69f4802a658e5143d9fdafb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a83f44d4611a4376bfd1045bb1ff7748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d52b61b05214308ab35b4f2af97ab35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e63f36301d24f09bd4c33274571ac4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "05e46d1f168344499eeb0f3390d0c6a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6140c99a6da44d989e5a2361c1da1a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4e0176f9ee44426bb80f10eb51b927e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2707b3fd597447d2905288caf4bee162",
              "IPY_MODEL_1c90cec486c647a5b62798d993bd672b",
              "IPY_MODEL_f2985b6f4f534f7ca2e8a977450c8382"
            ],
            "layout": "IPY_MODEL_035e328e3cd145c7aa0a4e87f5721e59"
          }
        },
        "2707b3fd597447d2905288caf4bee162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6061f80eab24af68d312d88e60e9bc5",
            "placeholder": "​",
            "style": "IPY_MODEL_84d4f33e2a2949bea63c91aebf4b5d53",
            "value": "config.json: 100%"
          }
        },
        "1c90cec486c647a5b62798d993bd672b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4fa1b8848b14084bebd85a59d348a87",
            "max": 6517,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16dac8db70ff4d5997f147abbe476422",
            "value": 6517
          }
        },
        "f2985b6f4f534f7ca2e8a977450c8382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe79260794a0453eba0bd3c6fca7b046",
            "placeholder": "​",
            "style": "IPY_MODEL_bbf0b892752a4ee5815a31ed7520e513",
            "value": " 6.52k/6.52k [00:00&lt;00:00, 105kB/s]"
          }
        },
        "035e328e3cd145c7aa0a4e87f5721e59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6061f80eab24af68d312d88e60e9bc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84d4f33e2a2949bea63c91aebf4b5d53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4fa1b8848b14084bebd85a59d348a87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16dac8db70ff4d5997f147abbe476422": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe79260794a0453eba0bd3c6fca7b046": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbf0b892752a4ee5815a31ed7520e513": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0034672b0aeb4374b5054443eb560d6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67433b8cf412451eb34c07d952acfe9c",
              "IPY_MODEL_ab36c74405264af997ca3d182d13a266",
              "IPY_MODEL_cb3aceef697b47c98f71e99770b387cf"
            ],
            "layout": "IPY_MODEL_4c873ae80b4f4c58bc8678cfc5b9c014"
          }
        },
        "67433b8cf412451eb34c07d952acfe9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74f7ac22cad3479595d23e771e75cdfa",
            "placeholder": "​",
            "style": "IPY_MODEL_dbb13d1948884ee698efaebd1a6babe8",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "ab36c74405264af997ca3d182d13a266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec7876c265e8428bbca6b77c0e884212",
            "max": 375045749,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15bbb28521734778a12fe866358b2542",
            "value": 375045749
          }
        },
        "cb3aceef697b47c98f71e99770b387cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43576e361a0a41ad866fc5d90da9e327",
            "placeholder": "​",
            "style": "IPY_MODEL_23e4802fb1f84d198cc22671ba234220",
            "value": " 375M/375M [00:02&lt;00:00, 116MB/s]"
          }
        },
        "4c873ae80b4f4c58bc8678cfc5b9c014": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74f7ac22cad3479595d23e771e75cdfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbb13d1948884ee698efaebd1a6babe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec7876c265e8428bbca6b77c0e884212": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15bbb28521734778a12fe866358b2542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43576e361a0a41ad866fc5d90da9e327": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23e4802fb1f84d198cc22671ba234220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}